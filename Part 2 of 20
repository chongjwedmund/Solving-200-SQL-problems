-- Source: Data Lemur 

-- App Click-through Rate (CTR) [Facebook SQL Interview Question]
-- Assume you have an events table on Facebook app analytics. 
-- Write a query to calculate the click-through rate (CTR) for the app in 2022 and round the results to 2 decimal places.

select app_id, 
round(100.0*sum(case when event_type = 'click' then 1 else 0 end) / 
sum(case when event_type = 'impression' then 1 else 0 end),2) ctr
from events 
where extract(year from timestamp::date) = 2022
group by app_id 

OR 

with cte_i as 
(select app_id, 
sum(case when event_type = 'impression' then 1 else 0 end) imp
from events 
where extract(year from timestamp::date) = 2022
group by app_id), 

cte_c as 
(select app_id, 
sum(case when event_type = 'click' then 1 else 0 end) cli
from events 
where extract(year from timestamp::date) = 2022
group by app_id)

select app_id, round(100.0*cli/imp,2) ctr
from cte_c 
join cte_i 
using(app_id) 

-- Second Day Confirmation [TikTok SQL Interview Question] 
-- Assume you're given tables with information about TikTok user sign-ups and confirmations through email and text. 
-- New users on TikTok sign up using their email addresses, and upon sign-up, each user receives a text message confirmation to activate their account.
-- Write a query to display the user IDs of those who did not confirm their sign-up on the first day, but confirmed on the second day.

select user_id 
from emails e 
join texts t
using (email_id)
where signup_action = 'Confirmed' 
and action_date::date - signup_date::date =1

OR 

with cte as 
(select user_id, action_date::date - signup_date::date datediff 
from emails e 
join texts t
using (email_id)
where signup_action = 'Confirmed') 

select user_id from cte 
where datediff = 1

OR 

select user_id
from emails e
join texts t  
using (email_id)
where date_part('day', action_date::date) - date_part('day', signup_date::date) = 1
and signup_action = 'Confirmed'

-- Cards Issued Difference [JPMorgan Chase SQL Interview Question]
-- Your team at JPMorgan Chase is preparing to launch a new credit card, and to gain some insights, you're analyzing how many credit cards were issued each month.
-- Write a query that outputs the name of each credit card and the difference in the number of issued cards between the month with the highest issuance cards and the lowest issuance. 
-- Arrange the results based on the largest disparity.

select card_name, max(issued_amount) - min(issued_amount) difference 
from monthly_cards_issued
group by card_name 
order by difference desc  

-- Compressed Mean [Alibaba SQL Interview Question]
-- You're trying to find the mean number of items per order on Alibaba, 
-- rounded to 1 decimal place using tables which includes information on the count of items in each order (item_count table) 
-- and the corresponding number of orders for each item count (order_occurrences table).

select round(1.0*sum(item_count * order_occurrences) /
sum(order_occurrences),1) mean 
from items_per_order 

-- Pharmacy Analytics (Part 1) [CVS Health SQL Interview Question]
-- CVS Health is trying to better understand its pharmacy sales, and how well different products are selling. 
-- Each drug can only be produced by one manufacturer.
-- Write a query to find the top 3 most profitable drugs sold, and how much profit they made. 
-- Assume that there are no ties in the profits. Display the result from the highest to the lowest total profit.

select drug, total_sales - cogs total_profit
from pharmacy_sales 
order by total_profit desc 
limit 3 

-- Pharmacy Analytics (Part 2) [CVS Health SQL Interview Question]
-- Write a query to identify the manufacturers associated with the drugs that resulted in losses for CVS Health and calculate the total amount of losses incurred.
-- Output the manufacturer's name, the number of drugs associated with losses, and the total losses in absolute value. 
-- Display the results sorted in descending order with the highest losses displayed at the top.

select manufacturer, count (distinct drug) drug_count, sum(cogs) - sum(total_sales) total_loss 
from pharmacy_sales 
where cogs > total_sales 
group by manufacturer
order by total_loss desc 

-- Pharmacy Analytics (Part 3) [CVS Health SQL Interview Question]
-- Write a query to calculate the total drug sales for each manufacturer. 
-- Round the answer to the nearest million and report your results in descending order of total sales. 
-- In case of any duplicates, sort them alphabetically by the manufacturer name.
-- Since this data will be displayed on a dashboard viewed by business stakeholders, please format your results as follows: "$36 million".

select manufacturer, 
concat('$', round(sum(total_sales)/10^6), ' million') sale 
from pharmacy_sales
group by manufacturer
order by sum(total_sales) desc  

-- User's Third Transaction [Uber SQL Interview Question]
-- Assume you are given the table below on Uber transactions made by users. 
-- Write a query to obtain the third transaction of every user. Output the user id, spend and transaction date.

with cte as 
(select *, 
row_number() over(partition by user_id order by transaction_date::date) rownum
from transactions) 

select user_id, spend, transaction_date 
from cte 
where rownum = 3 

OR 

select user_id, spend, transaction_date 
from (select *, 
row_number() over(partition by user_id order by transaction_date::date) rownum
from transactions) sq
where rownum = 3 

-- Sending vs. Opening Snaps [Snapchat SQL Interview Question]
-- Assume you're given tables with information on Snapchat users, including their ages and time spent sending and opening snaps.
-- Write a query to obtain a breakdown of the time spent sending vs. opening snaps as a percentage of total time spent on these activities grouped by age group. 
-- Round the percentage to 2 decimal places in the output.

with cte as 
(select age_bucket,
sum(case when activity_type = 'send' then time_spent else 0 end) as send_time,
sum(time_spent) total_time 
from activities a  
join age_breakdown b  
using (user_id)
where activity_type <> 'chat'
group by age_bucket)

select age_bucket, 
round(100.0*send_time / total_time,2) send_perc,
round(100.0*(total_time-send_time) / total_time,2) open_perc
from cte 

OR 

with send as 
(select age_bucket,
sum(case when activity_type = 'send' then time_spent else 0 end) as send_time
from activities a  
join age_breakdown b  
using (user_id)
group by age_bucket), 

total as 
(select age_bucket, sum(time_spent) total_time
from activities a  
join age_breakdown b 
using (user_id)
where activity_type <> 'chat'
group by age_bucket) 

select age_bucket, 
round(100.0*send_time / total_time,2) send_perc,
round(100.0*(total_time-send_time) / total_time,2) open_perc
from send
join total 
using (age_bucket)

OR 

with send as 
(select age_bucket, sum(time_spent) send_time
from activities a 
join age_breakdown b 
using(user_id)
where activity_type = 'send'
group by age_bucket),

open as 
(select age_bucket, sum(time_spent) open_time
from activities a 
join age_breakdown b 
using(user_id)
where activity_type = 'open'
group by age_bucket)

select s.age_bucket, 
round(100.0*send_time/(send_time + open_time),2) send_perc,
round(100.0*open_time/(send_time + open_time),2) open_perc

from send s 
join open o 
using(age_bucket) 

OR 

with cte as 
(select activity_type, age_bucket, sum(time_spent) time_spent_sum
from activities a
join age_breakdown ab 
using(user_id)
where activity_type in ('send','open')
group by activity_type, age_bucket)

select age_bucket, 

round(100.0*sum(case when activity_type = 'send' then time_spent_sum else 0 end) / 
sum(time_spent_sum),2) send_perc, 

round(100.0*sum(case when activity_type = 'open' then time_spent_sum else 0 end) / 
sum(time_spent_sum),2) open_perc 

from cte 
group by age_bucket 

-- Tweets' Rolling Averages [Twitter SQL Interview Question]
-- Given a table of tweet data over a specified time period, calculate the 3-day rolling average of tweets for each user. 
-- Output the user ID, tweet date, and rolling averages rounded to 2 decimal places.

select user_id, tweet_date, 
round(avg(tweet_count) over(partition by user_id order by tweet_date 
rows between 2 preceding and current row),2) rolling_avg_3days
from tweets
order by user_id, tweet_date 
